#BIBLIOTECAS USADAS: SKLEARN ,MATPLOTLIB, NUMPY E O ARQUIVO MAIN.PY ESTA SENDO IMPORTADO POIS SUAS VARIAVEIS
#SERAO UTILIZADAS NO CÓDIGO ABAIXO
#PARA BAIXAR AS BIBLIOTECAS USE: pip install numpy , pip install sklearn e pip install matplotlib
#TRABALHO FEITO POR MARCELO BENTO CÔGO E ARTHUR SANTOS ALMEIDA

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np
import main
import pandas as pd


#EXPLICACAO DO QUE SERA FEITO
# PARA REALIZAR O PCA PRIMEIRO TEMOS QUE DESCOBRIR A VARIANCIA DOS DADOS E VERIFICAR QUAIS COMPONENTES POSSUEM MAIOR VARIANCIA
#DESSA FORMA,LEMOS O ARQUIVO RAISIN_DATASET.CSV ,POIS APLICAREMOS O PCA NO CONJUNTO DE DADOS PARA DESCOBRIR O QUE EU DISSE ACIMA
# APOS DESCOBRIR QUANTOS COMPONENTES EXPLICAM A MAIOR PARTE DA VARIANCIA,APLICAMOS O PCA NO CONJUNTO DE TREINAMENTO.

#AQUI ESTAMOS NORMALIANDO OS DADOS PARA OS ATRIBUTOS TENHAM A MESMA IMPORTANCIA,OU SEJA,AGORA OS ATRIBUTOS TERAM O MESMO
#PESO PARA O PCA QUE IREMOS REALIZAR.
scaler_total = StandardScaler()
X_train_scaled = scaler_total.fit_transform(main.X)


# AO ESCOLHER N_COMPONENTES=NONE  O PCA IRA CALCULAR O NUMERO DE COMPONENTES BASEADO EM TODOS OS ATRIBUTOS DO DATASET.
# NESSE CASO A VARIAVEL X CRIADA NO ARQUIVO MAIN.PY SERA UTILIZADA POIS ELA CONTEM TODOS OS ATRIBUTOS QUE SAO:
# AREA, MAJORAXISLENGHT,MINORAXISLENGHT,ECCENTRICITY,CONVEXAREA,EXTENT E PERIMETER

pca = PCA(n_components=None)

#AO USAR O METODO FIT É USADO PARA SABER AS PRINCIPAIS DIRECOES DE VARIACAO OU SEJA OS COMPONENTES PRINCIPAIS DOS DADOS.
#DESSA FORA, O PCA ESTA AGORA COM AS DIREÇÕES DESSES COMPONENTES E COM ISSO PODEMOS CALCULAR SUA VARIANCIA PARA CADA COMPONENTE.
pca.fit(main.X)

#AQUI ESTAMOS OBTENDO A VARIANCIA EXPLICADA PARA CADA COMPONENTE
# SE VOCE PRINTAR A VARIAVEL EXPLAINED_VARIANCE VAI PERCEBER QUE APENAS 1 COMPONENTE POSSUI CERCA DE 98% DE EXPLICACAO DOS DADOS


explained_variance = pca.explained_variance_ratio_


# ESCOLHENDO O NUMERO DE COMPONENTE COM A VARIANCIA MAIS ALTA E ADICIANANDO O VALOR +1 POIS OS INDICES COMECAM COM O VALOR 0
n_components = np.argmax(explained_variance)+1


print("\n Número de componentes escolhidos:", n_components)

#O RESULTADO FOI 1,OU SEJA, APENAS UM COMPONENTE EXPLICA A MAIOR PARTE DA VARIANCIA DE DADOS,AGORA QUE DESCOBRIMOS ISSO
# APLICAMOS O PCA NO TREINAMENTO E DEPOIS PARTIMOS PARA O K-NN.
pca_treinamento = PCA(n_components = n_components)
X_train_pca = pca.fit_transform(main.X_train)


# O GRÁFICO DO X_TRAIN_PCA
# plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1])
# plt.xlabel("First Principal Component")
# plt.show()






